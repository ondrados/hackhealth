{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94398087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import zscore\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87953459",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8aafc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    samples, labels = zip(*((sample.transpose(1, 0), label)\n",
    "                            for (sample, label, *_) in batch))\n",
    "    samples = pad_sequence(\n",
    "        samples, batch_first=True).transpose(1, 2).contiguous()\n",
    "    return samples, torch.stack(labels)\n",
    "\n",
    "\n",
    "class AtrialStenosisDataset(Dataset):\n",
    "    def __init__(self, data_dir): # data_origin -> fnkv other option\n",
    "        \"\"\"\n",
    "        data_dir: data path\n",
    "        \"\"\"\n",
    "        self.data_dir = pickle.load(open(data_dir,'rb'))#pd.read_pickle(data_dir)\n",
    "        \n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.data_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        data = pd.read_csv(self.data_dir.iloc[idx].ecg_path)\n",
    "        data = data.drop(\" \", 1)\n",
    "        data = data.to_numpy()\n",
    "        \n",
    "        input = data\n",
    "\n",
    "        input = input.astype(np.float32)\n",
    "        input = zscore(input, axis=-1)         \n",
    "        input = torch.from_numpy(input)\n",
    "    \n",
    "        anot = self.data_dir.iloc[idx][\"as\"]\n",
    "        \n",
    "        return data, anot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b678932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AtrialStenosisDataset( data_dir = 'data/train.pkl')\n",
    "trainloader = DataLoader(train_dataset, batch_size=1, num_workers=4, shuffle=True, drop_last=True)\n",
    "\n",
    "valid_dataset = AtrialStenosisDataset( data_dir = 'data/test.pkl')\n",
    "validloader = DataLoader(valid_dataset, batch_size=1, num_workers=4, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'AtrialStenosisDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "data, anot = next(iter(trainloader))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3927bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Running on {device}..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74efae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.cnn1 = models.resnet18(pretrained=False)\n",
    "        self.fc1 = nn.Linear(2*1000, 2)\n",
    "\n",
    "\n",
    "    def forward(self, input1):\n",
    "        cnn_out1 = self.cnn1(input1)\n",
    "        output = self.fc1(cnn_out1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c414de96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qz/tnst848s0vz2c7b4sf96rql40000gn/T/ipykernel_54657/3765501210.py:26: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  data = data.drop(\" \", 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0/12727 - loss: 1.491615891456604, class: tensor([1])\n",
      "1: 0/12727 - loss: 1.0623211860656738, class: tensor([1])\n",
      "1: 1/12727 - loss: 0.6662098169326782, class: tensor([0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qz/tnst848s0vz2c7b4sf96rql40000gn/T/ipykernel_54657/521173992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/hackhealth/env/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/hackhealth/env/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from resnet import resnet50\n",
    "\n",
    "model = resnet50()\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "w_0 = 1.531\n",
    "w_1 = 8.576\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "class_weights = torch.FloatTensor([w_0, w_1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    running_loss_validation = 0\n",
    "    \n",
    "    for i,[data, anot] in enumerate(trainloader):\n",
    "        input = data.to(device=device).float()\n",
    "        anot = anot.to(device=device)\n",
    "        \n",
    "        input = input.permute(0, 2, 1)  \n",
    "        \n",
    "        if anot == 1:\n",
    "            in2 = input[:, :, 0:2500]\n",
    "            in2 = input[:, :, 2500:]\n",
    "            \n",
    "            for input in [in1 ,in2]:\n",
    "                \n",
    "                output = model(input)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = criterion(output, anot)\n",
    "                loss_item = loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss_item\n",
    "                \n",
    "                print(f\"{epoch}: {i}/{len(trainloader)} - loss: {loss_item}, class: {anot}\")\n",
    "        else:\n",
    "            output = model(input)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, anot)\n",
    "            loss_item = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss_item\n",
    "        \n",
    "            print(f\"{epoch}: {i}/{len(trainloader)} - loss: {loss_item}, class: {anot}\")\n",
    "        \n",
    "    loss_sum = running_loss/(i)\n",
    "    training_loss.append(loss_sum)\n",
    "    \n",
    "    loss_sum_valid = running_loss_validation/(j)\n",
    "    validation_loss.append(loss_sum_valid)\n",
    "    \n",
    "    torch.save(model.state_dict(),\"mymodels/weights/2Resnet\"+str(epoch)+\".pt\") #ukladanie modelu po každej epoche\n",
    "    #learnin curve\n",
    "    fig = plt.figure()\n",
    "    plt.plot(training_loss, label=\"training loss\")\n",
    "    plt.plot(validation_loss, label=\"validation loss\")\n",
    "    plt.title(\"Training loss \")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"2Training_loss.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2842490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3270"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36e89342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 2500])\n",
      "torch.Size([1, 12, 2500])\n"
     ]
    }
   ],
   "source": [
    "in1 = input[:, :, 0:2500]\n",
    "in2 = input[:, :, 2500:]\n",
    "print(in1.shape)\n",
    "print(in2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36313886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    image = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return image, targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackhealth",
   "language": "python",
   "name": "hackhealth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
